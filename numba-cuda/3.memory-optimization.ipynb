{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the memory performance of the GPU\n",
    "\n",
    "In this section we are going to investigate a crucial aspect of the memory locality on the GPUs. It should be perceived in a slightly different way than on the CPUs. To demonstrate this, we will use BLAS matrix-vector kernel using all the tricks we have learned so far. The threads of the GPU operate row-wise in the input matrix, each one taking care of a single row to compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CUDA transfer overheads: 0.2141221150904894\n",
      "  CUDA kernel time: 0.015257023811340331\n",
      "  Consumed memory bandwidth: 140.77095366421406 GB/s\n",
      "Total time (GPU): 0.23199772834777832 s\n",
      "Total time (CPU): 0.04511618614196777 s\n"
     ]
    }
   ],
   "source": [
    "import numba\n",
    "import numba.cuda as cuda\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "class time_region:\n",
    "    def __init__(self, time_offset=0):\n",
    "        self._time_off = time_offset\n",
    "\n",
    "    def __enter__(self):\n",
    "        self._t_start = time.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self._t_end = time.time()\n",
    "\n",
    "    def elapsed_time(self):\n",
    "        return self._time_off + (self._t_end - self._t_start)\n",
    "\n",
    "\n",
    "class time_region_cuda:\n",
    "    def __init__(self, time_offset=0, cuda_stream=0):\n",
    "        self._t_start = cuda.event(timing=True)\n",
    "        self._t_end = cuda.event(timing=True)\n",
    "        self._time_off = time_offset\n",
    "        self._cuda_stream = cuda_stream\n",
    "\n",
    "    def __enter__(self):\n",
    "        self._t_start.record(self._cuda_stream)\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self._t_end.record(self._cuda_stream)\n",
    "        self._t_end.synchronize()\n",
    "\n",
    "    def elapsed_time(self):\n",
    "        return self._time_off + 1.e-3*cuda.event_elapsed_time(self._t_start,\n",
    "                                                              self._t_end)\n",
    "\n",
    "\n",
    "@cuda.jit('void(float64, Array(float64, 2, \"C\"), Array(float64, 1, \"C\"), '\n",
    "          'float64, Array(float64, 1, \"C\"))')\n",
    "def _gemv_cuda(alpha, A, x, beta, y):\n",
    "    i = cuda.grid(1)\n",
    "    N, M = A.shape\n",
    "    if i >= N:\n",
    "        return\n",
    "\n",
    "    prod = 0.0\n",
    "    for j in range(M):\n",
    "        prod += A[i, j]*x[j]\n",
    "\n",
    "    y[i] = alpha*prod + beta*y[i]\n",
    "\n",
    "\n",
    "def gemv_gpu(alpha, A, x, beta, y):\n",
    "    # Works only for square matrices\n",
    "    N = A.shape[0]\n",
    "    with time_region_cuda() as t_xfer:\n",
    "        d_A = cuda.to_device(A)\n",
    "        d_x = cuda.to_device(x)\n",
    "        d_y = cuda.to_device(y)\n",
    "        y_ret = cuda.pinned_array(N)\n",
    "        \n",
    "    block_size = 128\n",
    "    num_blocks = N // block_size\n",
    "    if N % block_size:\n",
    "        num_blocks += 1\n",
    "\n",
    "    with time_region_cuda() as t_kernel:\n",
    "        _gemv_cuda[num_blocks, block_size](alpha, d_A, d_x, beta, d_y)\n",
    "\n",
    "    with time_region_cuda(t_xfer.elapsed_time()) as t_xfer:\n",
    "        d_y.copy_to_host(y_ret)\n",
    "\n",
    "    print(f'  CUDA transfer overheads: {t_xfer.elapsed_time()}')\n",
    "    print(f'  CUDA kernel time: {t_kernel.elapsed_time()}')\n",
    "    print(f'  Consumed memory bandwidth: {1e-9*8*N*(N+2)/t_kernel.elapsed_time()} GB/s')\n",
    "    return y_ret\n",
    "\n",
    "N = 1024*16\n",
    "A = np.random.rand(N, N)\n",
    "x = np.random.rand(N)\n",
    "y_orig = np.ones(N)\n",
    "alpha = 0.2\n",
    "beta = 1\n",
    "\n",
    "with time_region() as t_gpu:\n",
    "    y = gemv_gpu(alpha, A, x, beta, y_orig)\n",
    "\n",
    "with time_region() as t_ref:\n",
    "    y_ref = alpha*(A @ x) + beta*y_orig\n",
    "    \n",
    "    \n",
    "cuda.profile_stop()\n",
    "\n",
    "print(f'Total time (GPU): {t_gpu.elapsed_time()} s')\n",
    "print(f'Total time (CPU): {t_ref.elapsed_time()} s')\n",
    "\n",
    "assert np.allclose(y, y_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "> Increase the array size and record the CUDA kernel performance time. How much faster is it compared to the CPU version?\n",
    "\n",
    "As you might have noticed already, the GPU kernels is only about 2-3x faster than the CPU version. Not as good it has been with the vector addition kernel. Is this expected, is this how it should be? Let's look into the kernel in more detail.\n",
    "\n",
    "The kernel needs to read the whole matrix $A$ and the vectors $x$ and $y$, i.e., $8(N^2 + 2N)$ bytes need to be transferred to/from main memory in total. At the same time, the kernel performs $2N^2 + 3N$ floating point operations in total. This leads to an arithmetic intensity or flop:byte ratio equals to $\\frac{2N(N+\n",
    "\\frac{3}{2})}{8N(N+2)} \\approx 0.25$. This ratio is much higher than that for the vector addition kernel, but it is very low to make the kernel compute bound.\n",
    "\n",
    "> Given the nominal peak double precision performance (5.3 Tflop/s) and the nominal peak memory bandwidth of the P100 GPUs (732 GB/s), a kernel would need a flop:byte ratio of at least 7.24, so as to be compute bound.\n",
    "\n",
    "So, theoretically, we should be approaching the effective memory bandwidth limit of the device, but we only achieve 1/4 of it. The CPU kernel on the other hand seems to be optimal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU memory bandwidth consumed: 47.884390070318666\n"
     ]
    }
   ],
   "source": [
    "print(f'CPU memory bandwidth consumed: {1e-9*8*N*(N+2)/t_ref.elapsed_time()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is going on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miniconda-pythonhpc",
   "language": "python",
   "name": "miniconda-pythonhpc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
